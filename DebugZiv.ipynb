{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated random binary string:\n",
      "tensor([0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0.],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Generating a random binary string of length 50\n",
    "string_length = 50\n",
    "random_string = torch.randint(0, 2, (string_length,), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_kernel = 10  # Number of kernels\n",
    "M = 5            # Maximum kernel length\n",
    "eps = 1        # Epsilon for relu_distance calculation\n",
    "epochs = 1      # Number of epochs for optimization\n",
    "lr = 0.1        # Learning rate\n",
    "\n",
    "# Debug: Print the generated random string\n",
    "print(\"Generated random binary string:\")\n",
    "print(random_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seqs(nn.Module):\n",
    "    def __init__(self, dim, num_kernels):\n",
    "        super(Seqs, self).__init__()\n",
    "        self.dim = dim  # Kernel length\n",
    "        self.num_kernels = num_kernels  # Number of kernels\n",
    "        self.p = nn.Parameter(torch.rand(num_kernels, dim, dtype=torch.float32), requires_grad=True)\n",
    "\n",
    "    def forward(self):\n",
    "        # Debug: Check that the parameter p requires gradients\n",
    "        print(f\"Parameter p (before forward, requires_grad={self.p.requires_grad}):\\n{self.p}\")\n",
    "\n",
    "        # Forward computation\n",
    "        prob = self.p  # Kernel probabilities\n",
    "        logits = torch.stack((prob, 1 - prob), dim=-1)\n",
    "\n",
    "        # Debug: Check that logits is connected to the computation graph\n",
    "        print(f\"Logits grad_fn: {logits.grad_fn}\")\n",
    "\n",
    "        S = F.gumbel_softmax(logits, tau=0.5, hard=True, dim=-1)[..., 0]\n",
    "\n",
    "        # Debug: Check that S is connected to the computation graph\n",
    "        print(f\"Output S (grad_fn={S.grad_fn}):\\n{S}\")\n",
    "\n",
    "        return S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Example of usage during training\\nseq_model = Seqs(2, 3)  # Model with dimension 2 and 3 kernels\\noptimizer = torch.optim.Adam(seq_model.parameters(), lr=0.01)\\n\\n# Dummy input\\ninput_string = torch.randint(0, 2, (10,)).float()  # Random binary string\\n\\n# Forward pass\\noutput = seq_model()\\n\\n# Dummy loss\\nloss = output.sum()\\n\\n# Backward pass\\nloss.backward()\\n\\n# Debug: Gradient of p\\nprint(f\"Gradient of p after backward:\\n{seq_model.p.grad}\")'"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Example of usage during training\n",
    "seq_model = Seqs(2, 3)  # Model with dimension 2 and 3 kernels\n",
    "optimizer = torch.optim.Adam(seq_model.parameters(), lr=0.01)\n",
    "\n",
    "# Dummy input\n",
    "input_string = torch.randint(0, 2, (10,)).float()  # Random binary string\n",
    "\n",
    "# Forward pass\n",
    "output = seq_model()\n",
    "\n",
    "# Dummy loss\n",
    "loss = output.sum()\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Debug: Gradient of p\n",
    "print(f\"Gradient of p after backward:\\n{seq_model.p.grad}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[0.8202],\n",
      "        [0.7107],\n",
      "        [0.3711],\n",
      "        [0.5225],\n",
      "        [0.0151],\n",
      "        [0.4425],\n",
      "        [0.1490],\n",
      "        [0.6711],\n",
      "        [0.6322],\n",
      "        [0.9878]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[0.5792, 0.6206],\n",
      "        [0.6665, 0.6176],\n",
      "        [0.4997, 0.5996],\n",
      "        [0.5633, 0.2001],\n",
      "        [0.8886, 0.5704],\n",
      "        [0.2414, 0.4643],\n",
      "        [0.4607, 0.6583],\n",
      "        [0.8348, 0.0829],\n",
      "        [0.5344, 0.9818],\n",
      "        [0.0309, 0.8664]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[0.7717, 0.5145, 0.2694],\n",
      "        [0.0190, 0.9705, 0.2261],\n",
      "        [0.7313, 0.7303, 0.0300],\n",
      "        [0.9003, 0.1617, 0.7096],\n",
      "        [0.0268, 0.8553, 0.5031],\n",
      "        [0.2895, 0.5184, 0.9168],\n",
      "        [0.1977, 0.2972, 0.8944],\n",
      "        [0.3102, 0.3592, 0.6058],\n",
      "        [0.6112, 0.8595, 0.5041],\n",
      "        [0.0931, 0.8592, 0.2134]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[0.8459, 0.8337, 0.3831, 0.9423],\n",
      "        [0.4443, 0.6012, 0.6097, 0.3968],\n",
      "        [0.8404, 0.5390, 0.5535, 0.4054],\n",
      "        [0.9418, 0.1540, 0.9005, 0.9153],\n",
      "        [0.7279, 0.0870, 0.2970, 0.5919],\n",
      "        [0.9368, 0.9629, 0.9114, 0.1914],\n",
      "        [0.2462, 0.2371, 0.5745, 0.1585],\n",
      "        [0.8021, 0.6699, 0.5444, 0.2270],\n",
      "        [0.3233, 0.5953, 0.0754, 0.9164],\n",
      "        [0.9803, 0.7938, 0.8376, 0.7173]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[0.4239, 0.6768, 0.4311, 0.6832, 0.8666],\n",
      "        [0.6408, 0.4678, 0.1728, 0.1358, 0.4925],\n",
      "        [0.1218, 0.3995, 0.2057, 0.8080, 0.4366],\n",
      "        [0.4922, 0.3011, 0.2574, 0.5396, 0.8669],\n",
      "        [0.2921, 0.7770, 0.7649, 0.0054, 0.4051],\n",
      "        [0.9313, 0.2853, 0.9007, 0.5630, 0.6891],\n",
      "        [0.4492, 0.4069, 0.9826, 0.0489, 0.0275],\n",
      "        [0.4472, 0.0332, 0.3973, 0.6618, 0.8899],\n",
      "        [0.2210, 0.5937, 0.7437, 0.3799, 0.5793],\n",
      "        [0.2264, 0.6691, 0.7651, 0.5468, 0.6174]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[1., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Binary kernels (dim 1):\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<SelectBackward0>)\n",
      "Binary kernels (dim 2):\n",
      "tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Binary kernels (dim 3):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.]], grad_fn=<SelectBackward0>)\n",
      "Binary kernels (dim 4):\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "Binary kernels (dim 5):\n",
      "tensor([[1., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate a model for each dimension from 1 to M\n",
    "seq_models = [Seqs(dim, num_kernel) for dim in range(1, M + 1)]\n",
    "\n",
    "# Generate binary kernels directly from the model (without detach)\n",
    "binary_kernels = [model() for model in seq_models]\n",
    "\n",
    "# Debug: Display the generated binary kernels\n",
    "for dim, kernels in enumerate(binary_kernels, start=1):\n",
    "    print(f\"Binary kernels (dim {dim}):\")\n",
    "    print(kernels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patches(string, k):\n",
    "    # Manually slice the string to create patches while ensuring gradient flow\n",
    "    patches = [string[i:i + k].unsqueeze(0) for i in range(len(string) - k + 1)]\n",
    "    return torch.cat(patches, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZivEntropy(binary_kernels, string, eps=1):\n",
    "\n",
    "    ziv_probs = []\n",
    "\n",
    "    for k, kernels in enumerate(binary_kernels, start=1):\n",
    "        patches = create_patches(string, k)  # Replace unfold with custom function\n",
    "        print(f\"Patches (k={k}): requires_grad={patches.requires_grad}, shape={patches.shape}\")\n",
    "        print(f\"Binary kernels (dim {k}): {kernels}\")\n",
    "\n",
    "\n",
    "        # Manual calculation of Manhattan distances\n",
    "        distances = torch.abs(patches.unsqueeze(1) - kernels.unsqueeze(0)).sum(dim=-1)\n",
    "        print(f\"Distances (dim {k}): {distances}\")\n",
    "        print(f\"Distances grad_fn (dim {k}): {distances.grad_fn}\")\n",
    "\n",
    "        # Debug: Check that distances are part of the computational graph\n",
    "        print(f\"Manhattan distances (k={k}): requires_grad={distances.requires_grad}, grad_fn={distances.grad_fn}\")\n",
    "\n",
    "        # ReLU distance calculations\n",
    "        relu_distances = F.relu(eps - distances) / eps\n",
    "\n",
    "        # Debug: Check that relu_distances are connected to the graph\n",
    "        print(f\"ReLU distances (k={k}): requires_grad={relu_distances.requires_grad}, grad_fn={relu_distances.grad_fn}\")\n",
    "\n",
    "        # Probability calculations\n",
    "        probs = relu_distances.sum(dim=0) / (len(patches) * num_kernel)\n",
    "\n",
    "        # Debug: Check that probs are connected to the graph\n",
    "        print(f\"Probabilities (unnormalized) (k={k}): requires_grad={probs.requires_grad}, grad_fn={probs.grad_fn}\")\n",
    "\n",
    "        # Normalization of probabilities\n",
    "        normalized_probs = probs / probs.sum()\n",
    "\n",
    "        # Debug: Check that normalized_probs support gradients\n",
    "        print(f\"Normalized probabilities (k={k}): requires_grad={normalized_probs.requires_grad}, grad_fn={normalized_probs.grad_fn}\")\n",
    "\n",
    "        ziv_probs.append(normalized_probs)\n",
    "\n",
    "    return ziv_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Test of the ZivEntropy function with an example\\ndim = 3  # Kernel dimension\\nnum_kernels = 5  # Number of kernels\\nstring_length = 10  # Length of the binary string\\n\\n# Creating a Seqs model\\nseq_model = Seqs(dim, num_kernels)\\n\\n# Generating binary kernels\\nbinary_kernels = [seq_model()]  # Simulating one dimension with binary kernels\\n\\n# Debug: Check that the binary kernels are connected to the computation graph\\nfor kernel_index, kernel in enumerate(binary_kernels, start=1):\\n    print(f\"Binary kernel {kernel_index}: requires_grad={kernel.requires_grad}\")\\n\\n# Generating a random binary string\\nrandom_string = torch.randint(0, 2, (string_length,), dtype=torch.float32, requires_grad=True)\\n\\n# Debug: Check if the binary string requires gradients\\nprint(f\"Does the binary string require gradients? {\\'True\\' if random_string.requires_grad else \\'False\\'}\")\\n\\n# Epsilon\\neps = 0.5\\n\\n# Test of the ZivEntropy function\\nziv_probs = ZivEntropy(binary_kernels, random_string, eps=eps)\\n\\n# Debug: Print probabilities to verify connection to the computation graph\\nfor k, probs in enumerate(ziv_probs, start=1):\\n    print(f\"Normalized probabilities (dim={k}):\\n{probs}\")\\n    print(f\"Probabilities grad_fn: {probs.grad_fn}\")\\n    print(f\"Do probabilities (dim={k}) require gradients? {\\'True\\' if probs.requires_grad else \\'False\\'}\")'"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Test of the ZivEntropy function with an example\n",
    "dim = 3  # Kernel dimension\n",
    "num_kernels = 5  # Number of kernels\n",
    "string_length = 10  # Length of the binary string\n",
    "\n",
    "# Creating a Seqs model\n",
    "seq_model = Seqs(dim, num_kernels)\n",
    "\n",
    "# Generating binary kernels\n",
    "binary_kernels = [seq_model()]  # Simulating one dimension with binary kernels\n",
    "\n",
    "# Debug: Check that the binary kernels are connected to the computation graph\n",
    "for kernel_index, kernel in enumerate(binary_kernels, start=1):\n",
    "    print(f\"Binary kernel {kernel_index}: requires_grad={kernel.requires_grad}\")\n",
    "\n",
    "# Generating a random binary string\n",
    "random_string = torch.randint(0, 2, (string_length,), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Debug: Check if the binary string requires gradients\n",
    "print(f\"Does the binary string require gradients? {'True' if random_string.requires_grad else 'False'}\")\n",
    "\n",
    "# Epsilon\n",
    "eps = 0.5\n",
    "\n",
    "# Test of the ZivEntropy function\n",
    "ziv_probs = ZivEntropy(binary_kernels, random_string, eps=eps)\n",
    "\n",
    "# Debug: Print probabilities to verify connection to the computation graph\n",
    "for k, probs in enumerate(ziv_probs, start=1):\n",
    "    print(f\"Normalized probabilities (dim={k}):\\n{probs}\")\n",
    "    print(f\"Probabilities grad_fn: {probs.grad_fn}\")\n",
    "    print(f\"Do probabilities (dim={k}) require gradients? {'True' if probs.requires_grad else 'False'}\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZivEntropyLoss(ziv_probs):\n",
    "    entropies_per_dim = []\n",
    "\n",
    "    for probs in ziv_probs:\n",
    "        probs = probs + 1e-8  # Avoid log(0)\n",
    "        H_values = -(probs * probs.log())\n",
    "        H_mean = H_values.mean()\n",
    "        entropies_per_dim.append(H_mean)\n",
    "\n",
    "    full_entropy = torch.stack(entropies_per_dim).sum()\n",
    "    return full_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Test of the ZivEntropyLoss function\\n\\n# Simulate normalized probabilities generated by ZivEntropy\\nziv_probs = [\\n    torch.tensor([0.2, 0.3, 0.5], requires_grad=True),  # Probabilities for dim=1\\n    torch.tensor([0.1, 0.2, 0.4, 0.3], requires_grad=True),  # Probabilities for dim=2\\n    torch.tensor([0.3, 0.3, 0.2, 0.2], requires_grad=True)  # Probabilities for dim=3\\n]\\n\\n# Debug: Check if tensors in ziv_probs require gradients\\nfor dim_index, probs in enumerate(ziv_probs, start=1):\\n    print(f\"Dimension {dim_index}: requires_grad={probs.requires_grad}\")\\n\\n# Compute the loss\\nfull_entropy = ZivEntropyLoss(ziv_probs)\\n\\n# Debug: Check connection to the computation graph for full_entropy\\nprint(f\"Does Full Entropy require gradients? {\\'True\\' if full_entropy.grad_fn is not None else \\'False\\'}\")\\n\\n# Backpropagation to verify gradients\\nfull_entropy.backward()\\n\\n# Debug: Print gradients for each dimension\\nfor dim_index, probs in enumerate(ziv_probs, start=1):\\n    print(f\"Gradient for probabilities of dimension {dim_index}: {probs.grad}\")\\n    print(f\"Is gradient computed for dim {dim_index}? {\\'True\\' if probs.grad is not None else \\'False\\'}\")'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Test of the ZivEntropyLoss function\n",
    "\n",
    "# Simulate normalized probabilities generated by ZivEntropy\n",
    "ziv_probs = [\n",
    "    torch.tensor([0.2, 0.3, 0.5], requires_grad=True),  # Probabilities for dim=1\n",
    "    torch.tensor([0.1, 0.2, 0.4, 0.3], requires_grad=True),  # Probabilities for dim=2\n",
    "    torch.tensor([0.3, 0.3, 0.2, 0.2], requires_grad=True)  # Probabilities for dim=3\n",
    "]\n",
    "\n",
    "# Debug: Check if tensors in ziv_probs require gradients\n",
    "for dim_index, probs in enumerate(ziv_probs, start=1):\n",
    "    print(f\"Dimension {dim_index}: requires_grad={probs.requires_grad}\")\n",
    "\n",
    "# Compute the loss\n",
    "full_entropy = ZivEntropyLoss(ziv_probs)\n",
    "\n",
    "# Debug: Check connection to the computation graph for full_entropy\n",
    "print(f\"Does Full Entropy require gradients? {'True' if full_entropy.grad_fn is not None else 'False'}\")\n",
    "\n",
    "# Backpropagation to verify gradients\n",
    "full_entropy.backward()\n",
    "\n",
    "# Debug: Print gradients for each dimension\n",
    "for dim_index, probs in enumerate(ziv_probs, start=1):\n",
    "    print(f\"Gradient for probabilities of dimension {dim_index}: {probs.grad}\")\n",
    "    print(f\"Is gradient computed for dim {dim_index}? {'True' if probs.grad is not None else 'False'}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patches (k=1): requires_grad=True, shape=torch.Size([50, 1])\n",
      "Binary kernels (dim 1): tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], grad_fn=<SelectBackward0>)\n",
      "Distances (dim 1): tensor([[0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.],\n",
      "        [1., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 1., 0., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)\n",
      "Distances grad_fn (dim 1): <SumBackward1 object at 0x1755ad610>\n",
      "Manhattan distances (k=1): requires_grad=True, grad_fn=<SumBackward1 object at 0x1755ad610>\n",
      "ReLU distances (k=1): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Probabilities (unnormalized) (k=1): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Normalized probabilities (k=1): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Patches (k=2): requires_grad=True, shape=torch.Size([49, 2])\n",
      "Binary kernels (dim 2): tensor([[0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Distances (dim 2): tensor([[0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [1., 1., 2., 2., 1., 1., 2., 2., 2., 2.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [1., 1., 2., 2., 1., 1., 2., 2., 2., 2.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [1., 1., 2., 2., 1., 1., 2., 2., 2., 2.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [1., 1., 2., 2., 1., 1., 2., 2., 2., 2.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 1., 1., 0., 0., 0., 0.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 0., 0., 1., 1., 1., 1.],\n",
      "        [2., 2., 1., 1., 2., 2., 1., 1., 1., 1.]], grad_fn=<SumBackward1>)\n",
      "Distances grad_fn (dim 2): <SumBackward1 object at 0x1755ad970>\n",
      "Manhattan distances (k=2): requires_grad=True, grad_fn=<SumBackward1 object at 0x1755ad970>\n",
      "ReLU distances (k=2): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad970>\n",
      "Probabilities (unnormalized) (k=2): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad970>\n",
      "Normalized probabilities (k=2): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad970>\n",
      "Patches (k=3): requires_grad=True, shape=torch.Size([48, 3])\n",
      "Binary kernels (dim 3): tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 0.]], grad_fn=<SelectBackward0>)\n",
      "Distances (dim 3): tensor([[3., 1., 2., 1., 0., 1., 2., 0., 0., 2.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [1., 1., 2., 1., 2., 1., 2., 2., 2., 2.],\n",
      "        [1., 3., 2., 3., 2., 3., 0., 2., 2., 2.],\n",
      "        [3., 1., 2., 1., 0., 1., 2., 0., 0., 2.],\n",
      "        [1., 1., 2., 1., 2., 1., 2., 2., 2., 2.],\n",
      "        [0., 2., 1., 2., 3., 2., 1., 3., 3., 1.],\n",
      "        [2., 2., 1., 2., 1., 2., 1., 1., 1., 1.],\n",
      "        [2., 0., 1., 0., 1., 0., 3., 1., 1., 1.],\n",
      "        [0., 2., 1., 2., 3., 2., 1., 3., 3., 1.],\n",
      "        [2., 2., 1., 2., 1., 2., 1., 1., 1., 1.],\n",
      "        [3., 1., 2., 1., 0., 1., 2., 0., 0., 2.],\n",
      "        [1., 1., 2., 1., 2., 1., 2., 2., 2., 2.],\n",
      "        [1., 3., 2., 3., 2., 3., 0., 2., 2., 2.],\n",
      "        [3., 1., 2., 1., 0., 1., 2., 0., 0., 2.],\n",
      "        [1., 1., 2., 1., 2., 1., 2., 2., 2., 2.],\n",
      "        [1., 3., 2., 3., 2., 3., 0., 2., 2., 2.],\n",
      "        [2., 0., 1., 0., 1., 0., 3., 1., 1., 1.],\n",
      "        [0., 2., 1., 2., 3., 2., 1., 3., 3., 1.],\n",
      "        [2., 2., 1., 2., 1., 2., 1., 1., 1., 1.],\n",
      "        [3., 1., 2., 1., 0., 1., 2., 0., 0., 2.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [1., 1., 2., 1., 2., 1., 2., 2., 2., 2.],\n",
      "        [0., 2., 1., 2., 3., 2., 1., 3., 3., 1.],\n",
      "        [2., 2., 1., 2., 1., 2., 1., 1., 1., 1.],\n",
      "        [2., 0., 1., 0., 1., 0., 3., 1., 1., 1.],\n",
      "        [1., 3., 2., 3., 2., 3., 0., 2., 2., 2.],\n",
      "        [3., 1., 2., 1., 0., 1., 2., 0., 0., 2.],\n",
      "        [1., 1., 2., 1., 2., 1., 2., 2., 2., 2.],\n",
      "        [1., 3., 2., 3., 2., 3., 0., 2., 2., 2.],\n",
      "        [3., 1., 2., 1., 0., 1., 2., 0., 0., 2.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [1., 1., 2., 1., 2., 1., 2., 2., 2., 2.],\n",
      "        [1., 3., 2., 3., 2., 3., 0., 2., 2., 2.],\n",
      "        [3., 1., 2., 1., 0., 1., 2., 0., 0., 2.],\n",
      "        [2., 2., 3., 2., 1., 2., 1., 1., 1., 3.],\n",
      "        [1., 1., 2., 1., 2., 1., 2., 2., 2., 2.],\n",
      "        [1., 3., 2., 3., 2., 3., 0., 2., 2., 2.],\n",
      "        [2., 0., 1., 0., 1., 0., 3., 1., 1., 1.],\n",
      "        [1., 3., 2., 3., 2., 3., 0., 2., 2., 2.],\n",
      "        [2., 0., 1., 0., 1., 0., 3., 1., 1., 1.]], grad_fn=<SumBackward1>)\n",
      "Distances grad_fn (dim 3): <SumBackward1 object at 0x1755ad610>\n",
      "Manhattan distances (k=3): requires_grad=True, grad_fn=<SumBackward1 object at 0x1755ad610>\n",
      "ReLU distances (k=3): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Probabilities (unnormalized) (k=3): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Normalized probabilities (k=3): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Patches (k=4): requires_grad=True, shape=torch.Size([47, 4])\n",
      "Binary kernels (dim 4): tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [1., 1., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "Distances (dim 4): tensor([[1., 2., 1., 2., 0., 3., 2., 1., 2., 2.],\n",
      "        [1., 0., 1., 2., 2., 1., 2., 1., 2., 0.],\n",
      "        [1., 2., 3., 2., 2., 1., 4., 1., 2., 2.],\n",
      "        [1., 2., 3., 0., 2., 3., 2., 1., 4., 2.],\n",
      "        [2., 1., 0., 3., 1., 2., 1., 2., 1., 1.],\n",
      "        [2., 1., 2., 3., 3., 0., 3., 2., 1., 1.],\n",
      "        [2., 3., 4., 1., 3., 2., 3., 2., 3., 3.],\n",
      "        [3., 2., 1., 2., 2., 3., 0., 3., 2., 2.],\n",
      "        [3., 2., 1., 4., 2., 1., 2., 3., 0., 2.],\n",
      "        [2., 3., 4., 1., 3., 2., 3., 2., 3., 3.],\n",
      "        [2., 3., 2., 1., 1., 4., 1., 2., 3., 3.],\n",
      "        [2., 1., 0., 3., 1., 2., 1., 2., 1., 1.],\n",
      "        [1., 2., 3., 2., 2., 1., 4., 1., 2., 2.],\n",
      "        [1., 2., 3., 0., 2., 3., 2., 1., 4., 2.],\n",
      "        [2., 1., 0., 3., 1., 2., 1., 2., 1., 1.],\n",
      "        [1., 2., 3., 2., 2., 1., 4., 1., 2., 2.],\n",
      "        [2., 1., 2., 1., 3., 2., 1., 2., 3., 1.],\n",
      "        [3., 2., 1., 4., 2., 1., 2., 3., 0., 2.],\n",
      "        [2., 3., 4., 1., 3., 2., 3., 2., 3., 3.],\n",
      "        [2., 3., 2., 1., 1., 4., 1., 2., 3., 3.],\n",
      "        [1., 2., 1., 2., 0., 3., 2., 1., 2., 2.],\n",
      "        [0., 1., 2., 1., 1., 2., 3., 0., 3., 1.],\n",
      "        [1., 0., 1., 2., 2., 1., 2., 1., 2., 0.],\n",
      "        [2., 1., 2., 3., 3., 0., 3., 2., 1., 1.],\n",
      "        [2., 3., 4., 1., 3., 2., 3., 2., 3., 3.],\n",
      "        [3., 2., 1., 2., 2., 3., 0., 3., 2., 2.],\n",
      "        [2., 3., 2., 3., 1., 2., 3., 2., 1., 3.],\n",
      "        [1., 2., 3., 0., 2., 3., 2., 1., 4., 2.],\n",
      "        [2., 1., 0., 3., 1., 2., 1., 2., 1., 1.],\n",
      "        [1., 2., 3., 2., 2., 1., 4., 1., 2., 2.],\n",
      "        [1., 2., 3., 0., 2., 3., 2., 1., 4., 2.],\n",
      "        [1., 2., 1., 2., 0., 3., 2., 1., 2., 2.],\n",
      "        [0., 1., 2., 1., 1., 2., 3., 0., 3., 1.],\n",
      "        [0., 1., 2., 1., 1., 2., 3., 0., 3., 1.],\n",
      "        [0., 1., 2., 1., 1., 2., 3., 0., 3., 1.],\n",
      "        [0., 1., 2., 1., 1., 2., 3., 0., 3., 1.],\n",
      "        [0., 1., 2., 1., 1., 2., 3., 0., 3., 1.],\n",
      "        [0., 1., 2., 1., 1., 2., 3., 0., 3., 1.],\n",
      "        [1., 0., 1., 2., 2., 1., 2., 1., 2., 0.],\n",
      "        [1., 2., 3., 2., 2., 1., 4., 1., 2., 2.],\n",
      "        [1., 2., 3., 0., 2., 3., 2., 1., 4., 2.],\n",
      "        [1., 2., 1., 2., 0., 3., 2., 1., 2., 2.],\n",
      "        [1., 0., 1., 2., 2., 1., 2., 1., 2., 0.],\n",
      "        [1., 2., 3., 2., 2., 1., 4., 1., 2., 2.],\n",
      "        [2., 1., 2., 1., 3., 2., 1., 2., 3., 1.],\n",
      "        [2., 3., 2., 3., 1., 2., 3., 2., 1., 3.],\n",
      "        [2., 1., 2., 1., 3., 2., 1., 2., 3., 1.]], grad_fn=<SumBackward1>)\n",
      "Distances grad_fn (dim 4): <SumBackward1 object at 0x1755ad970>\n",
      "Manhattan distances (k=4): requires_grad=True, grad_fn=<SumBackward1 object at 0x1755ad970>\n",
      "ReLU distances (k=4): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad970>\n",
      "Probabilities (unnormalized) (k=4): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad970>\n",
      "Normalized probabilities (k=4): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad970>\n",
      "Patches (k=5): requires_grad=True, shape=torch.Size([46, 5])\n",
      "Binary kernels (dim 5): tensor([[1., 1., 0., 1., 1.],\n",
      "        [1., 0., 0., 1., 1.],\n",
      "        [1., 1., 0., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [1., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Distances (dim 5): tensor([[3., 4., 3., 2., 1., 2., 2., 4., 3., 3.],\n",
      "        [2., 3., 2., 3., 2., 1., 3., 1., 4., 4.],\n",
      "        [0., 1., 0., 3., 2., 1., 3., 3., 2., 2.],\n",
      "        [3., 2., 3., 4., 3., 2., 4., 2., 3., 3.],\n",
      "        [4., 5., 4., 1., 2., 3., 1., 3., 4., 4.],\n",
      "        [1., 2., 1., 2., 3., 2., 2., 2., 3., 3.],\n",
      "        [2., 1., 2., 3., 4., 3., 3., 3., 2., 2.],\n",
      "        [5., 4., 5., 2., 3., 4., 2., 2., 3., 3.],\n",
      "        [2., 3., 2., 1., 2., 3., 1., 3., 2., 2.],\n",
      "        [1., 0., 1., 4., 3., 2., 4., 2., 1., 1.],\n",
      "        [4., 3., 4., 3., 2., 3., 3., 3., 2., 2.],\n",
      "        [3., 4., 3., 2., 1., 2., 2., 2., 3., 3.],\n",
      "        [0., 1., 0., 3., 2., 1., 3., 3., 2., 2.],\n",
      "        [3., 2., 3., 4., 3., 2., 4., 2., 3., 3.],\n",
      "        [3., 4., 3., 2., 1., 2., 2., 2., 3., 3.],\n",
      "        [1., 2., 1., 2., 3., 2., 2., 4., 3., 3.],\n",
      "        [4., 3., 4., 3., 4., 3., 3., 1., 4., 4.],\n",
      "        [2., 3., 2., 1., 2., 3., 1., 3., 2., 2.],\n",
      "        [1., 0., 1., 4., 3., 2., 4., 2., 1., 1.],\n",
      "        [3., 2., 3., 4., 1., 2., 4., 2., 1., 1.],\n",
      "        [2., 3., 2., 3., 0., 1., 3., 3., 2., 2.],\n",
      "        [2., 3., 2., 3., 2., 1., 3., 3., 4., 4.],\n",
      "        [3., 4., 3., 2., 3., 2., 2., 2., 5., 5.],\n",
      "        [1., 2., 1., 2., 3., 2., 2., 2., 3., 3.],\n",
      "        [2., 1., 2., 3., 4., 3., 3., 3., 2., 2.],\n",
      "        [4., 3., 4., 3., 2., 3., 3., 1., 2., 2.],\n",
      "        [1., 2., 1., 2., 1., 2., 2., 4., 1., 1.],\n",
      "        [3., 2., 3., 4., 3., 2., 4., 2., 3., 3.],\n",
      "        [3., 4., 3., 2., 1., 2., 2., 2., 3., 3.],\n",
      "        [0., 1., 0., 3., 2., 1., 3., 3., 2., 2.],\n",
      "        [2., 1., 2., 5., 2., 1., 5., 1., 2., 2.],\n",
      "        [2., 3., 2., 3., 0., 1., 3., 3., 2., 2.],\n",
      "        [1., 2., 1., 4., 1., 0., 4., 2., 3., 3.],\n",
      "        [1., 2., 1., 4., 1., 0., 4., 2., 3., 3.],\n",
      "        [1., 2., 1., 4., 1., 0., 4., 2., 3., 3.],\n",
      "        [1., 2., 1., 4., 1., 0., 4., 2., 3., 3.],\n",
      "        [1., 2., 1., 4., 1., 0., 4., 2., 3., 3.],\n",
      "        [2., 3., 2., 3., 2., 1., 3., 3., 4., 4.],\n",
      "        [2., 3., 2., 3., 2., 1., 3., 1., 4., 4.],\n",
      "        [0., 1., 0., 3., 2., 1., 3., 3., 2., 2.],\n",
      "        [2., 1., 2., 5., 2., 1., 5., 1., 2., 2.],\n",
      "        [3., 4., 3., 2., 1., 2., 2., 4., 3., 3.],\n",
      "        [2., 3., 2., 3., 2., 1., 3., 1., 4., 4.],\n",
      "        [1., 2., 1., 2., 3., 2., 2., 4., 3., 3.],\n",
      "        [3., 2., 3., 4., 3., 2., 4., 0., 3., 3.],\n",
      "        [2., 3., 2., 1., 2., 3., 1., 5., 2., 2.]], grad_fn=<SumBackward1>)\n",
      "Distances grad_fn (dim 5): <SumBackward1 object at 0x1755ad610>\n",
      "Manhattan distances (k=5): requires_grad=True, grad_fn=<SumBackward1 object at 0x1755ad610>\n",
      "ReLU distances (k=5): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Probabilities (unnormalized) (k=5): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Normalized probabilities (k=5): requires_grad=True, grad_fn=<DivBackward0 object at 0x1755ad610>\n",
      "Gradient magnitude for kernel (dim 1): 0.0\n",
      "Gradient magnitude for kernel (dim 2): 0.0\n",
      "Gradient magnitude for kernel (dim 3): 0.0\n",
      "Gradient magnitude for kernel (dim 4): 0.0\n",
      "Gradient magnitude for kernel (dim 5): 0.0\n",
      "Full Entropy (Epoch 1): 1.0488613843917847\n",
      "Kernel gradients (dim 1):\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]])\n",
      "Kernel gradients (dim 2):\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "Kernel gradients (dim 3):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Kernel gradients (dim 4):\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "Kernel gradients (dim 5):\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Optimizer for the model parameters\n",
    "#optimizer = torch.optim.Adam([param for model in seq_models for param in model.parameters()], lr=lr)\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': seq_models[dim - 1].parameters(), 'lr': lr * (5 / dim)} for dim in range(1, 6)\n",
    "])\n",
    "\n",
    "\n",
    "losses = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    \n",
    "    # Calculate probabilities using ZivEntropy\n",
    "    ziv_probs = ZivEntropy(binary_kernels, random_string, eps=eps)\n",
    "\n",
    "    # Calculate the loss\n",
    "    full_entropy = ZivEntropyLoss(ziv_probs)\n",
    "\n",
    "    # Save the loss to the list\n",
    "    losses.append(full_entropy.item())\n",
    "\n",
    "    # Backpropagation\n",
    "    full_entropy.backward(retain_graph=True)\n",
    "\n",
    "    for dim, model in enumerate(seq_models, start=1):\n",
    "        print(f\"Gradient magnitude for kernel (dim {dim}): {model.p.grad.abs().mean().item()}\")\n",
    "\n",
    "    # Debug: Display the total entropy\n",
    "    print(f\"Full Entropy (Epoch {epoch + 1}): {full_entropy.item()}\")\n",
    "\n",
    "    # Debug: Gradients for each dimension\n",
    "    for dim, model in enumerate(seq_models, start=1):\n",
    "        print(f\"Kernel gradients (dim {dim}):\\n{model.p.grad}\")\n",
    "\n",
    "    # Update the parameters\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), losses, marker='o', linestyle='-', color='b')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Full Entropy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Gradient magnitude for kernel (dim 1): 0.009999998845160007\n",
      "Test Gradient magnitude for kernel (dim 2): 0.009999998845160007\n",
      "Test Gradient magnitude for kernel (dim 3): 0.009999999776482582\n",
      "Test Gradient magnitude for kernel (dim 4): 0.009999999776482582\n",
      "Test Gradient magnitude for kernel (dim 5): 0.009999999776482582\n"
     ]
    }
   ],
   "source": [
    "test_loss = full_entropy + 0.01 * sum([model.p.sum() for model in seq_models])\n",
    "test_loss.backward()\n",
    "\n",
    "for dim, model in enumerate(seq_models, start=1):\n",
    "    print(f\"Test Gradient magnitude for kernel (dim {dim}): {model.p.grad.abs().mean().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel 1 requires_grad: True\n",
      "Kernel 2 requires_grad: True\n",
      "Kernel 3 requires_grad: True\n",
      "Kernel 4 requires_grad: True\n",
      "Kernel 5 requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "for i, model in enumerate(seq_models):\n",
    "    print(f\"Kernel {i + 1} requires_grad: {model.p.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test gradient for kernel 5: tensor([[1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100],\n",
      "        [1.0100, 1.0100, 1.0100, 1.0100, 1.0100]])\n"
     ]
    }
   ],
   "source": [
    "test_loss = seq_models[4].p.sum()  # Compute the sum of the parameter p for the model with dimension 5\n",
    "test_loss.backward()  # Backpropagation to calculate gradients\n",
    "\n",
    "# Print the gradient of the parameter p for the model with dimension 5\n",
    "print(f\"Test gradient for kernel 5: {seq_models[4].p.grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated values for the kernel with dimension 5 (p):\n",
      "tensor([[ 0.3495,  0.6024,  0.3567,  0.6088,  0.7922],\n",
      "        [ 0.5664,  0.3934,  0.0984,  0.0614,  0.4181],\n",
      "        [ 0.0474,  0.3251,  0.1313,  0.7335,  0.3622],\n",
      "        [ 0.4177,  0.2267,  0.1830,  0.4652,  0.7925],\n",
      "        [ 0.2176,  0.7025,  0.6905, -0.0690,  0.3307],\n",
      "        [ 0.8569,  0.2109,  0.8263,  0.4886,  0.6147],\n",
      "        [ 0.3748,  0.3324,  0.9082, -0.0255, -0.0469],\n",
      "        [ 0.3728, -0.0412,  0.3229,  0.5874,  0.8155],\n",
      "        [ 0.1466,  0.5193,  0.6693,  0.3055,  0.5049],\n",
      "        [ 0.1520,  0.5947,  0.6907,  0.4723,  0.5430]])\n"
     ]
    }
   ],
   "source": [
    "# Optimization\n",
    "optimizer.step()  # Update parameters\n",
    "\n",
    "# Check the updated kernel values for the model with dimension 5\n",
    "print(f\"Updated values for the kernel with dimension 5 (p):\\n{seq_models[4].p.data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.4482],\n",
      "        [ 0.3387],\n",
      "        [-0.0010],\n",
      "        [ 0.1505],\n",
      "        [-0.3569],\n",
      "        [ 0.0705],\n",
      "        [-0.2231],\n",
      "        [ 0.2990],\n",
      "        [ 0.2601],\n",
      "        [ 0.6158]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x1755b9e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x1755b9e50>):\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.3932,  0.4345],\n",
      "        [ 0.4805,  0.4316],\n",
      "        [ 0.3137,  0.4136],\n",
      "        [ 0.3773,  0.0141],\n",
      "        [ 0.7026,  0.3844],\n",
      "        [ 0.0554,  0.2783],\n",
      "        [ 0.2746,  0.4723],\n",
      "        [ 0.6488, -0.1031],\n",
      "        [ 0.3484,  0.7957],\n",
      "        [-0.1551,  0.6804]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x1755b9e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x1755b9e50>):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [1., 0.],\n",
      "        [0., 1.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.6477,  0.3905,  0.1454],\n",
      "        [-0.1050,  0.8465,  0.1021],\n",
      "        [ 0.6073,  0.6063, -0.0940],\n",
      "        [ 0.7762,  0.0377,  0.5855],\n",
      "        [-0.0972,  0.7313,  0.3791],\n",
      "        [ 0.1655,  0.3944,  0.7928],\n",
      "        [ 0.0737,  0.1732,  0.7704],\n",
      "        [ 0.1862,  0.2352,  0.4817],\n",
      "        [ 0.4872,  0.7355,  0.3801],\n",
      "        [-0.0309,  0.7352,  0.0894]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x1755b9e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x1755b9e50>):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.7528,  0.7407,  0.2901,  0.8493],\n",
      "        [ 0.3513,  0.5082,  0.5167,  0.3038],\n",
      "        [ 0.7474,  0.4460,  0.4605,  0.3124],\n",
      "        [ 0.8488,  0.0610,  0.8075,  0.8223],\n",
      "        [ 0.6349, -0.0060,  0.2039,  0.4989],\n",
      "        [ 0.8438,  0.8699,  0.8184,  0.0984],\n",
      "        [ 0.1531,  0.1441,  0.4814,  0.0654],\n",
      "        [ 0.7091,  0.5769,  0.4514,  0.1340],\n",
      "        [ 0.2302,  0.5023, -0.0176,  0.8233],\n",
      "        [ 0.8873,  0.7008,  0.7446,  0.6243]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x1755b9e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x1755b9e50>):\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 1., 0., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [0., 1., 1., 0.],\n",
      "        [0., 1., 1., 1.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [0., 1., 0., 1.],\n",
      "        [1., 1., 0., 1.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.3495,  0.6024,  0.3567,  0.6088,  0.7922],\n",
      "        [ 0.5664,  0.3934,  0.0984,  0.0614,  0.4181],\n",
      "        [ 0.0474,  0.3251,  0.1313,  0.7335,  0.3622],\n",
      "        [ 0.4177,  0.2267,  0.1830,  0.4652,  0.7925],\n",
      "        [ 0.2176,  0.7025,  0.6905, -0.0690,  0.3307],\n",
      "        [ 0.8569,  0.2109,  0.8263,  0.4886,  0.6147],\n",
      "        [ 0.3748,  0.3324,  0.9082, -0.0255, -0.0469],\n",
      "        [ 0.3728, -0.0412,  0.3229,  0.5874,  0.8155],\n",
      "        [ 0.1466,  0.5193,  0.6693,  0.3055,  0.5049],\n",
      "        [ 0.1520,  0.5947,  0.6907,  0.4723,  0.5430]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x1755b9e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x1755b9e50>):\n",
      "tensor([[1., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.]], grad_fn=<SelectBackward0>)\n",
      "Generated binary kernel for Dim 5:\n",
      "tensor([[1., 0., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 0., 0.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1., 0.],\n",
      "        [1., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 0., 0., 0.]], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply forward pass for all models\n",
    "binary_kernels = [seq_model() for seq_model in seq_models]\n",
    "\n",
    "# Print the binary kernel result corresponding to dimension 5\n",
    "print(\"Generated binary kernel for Dim 5:\")\n",
    "print(binary_kernels[4])  # The kernel for dimension 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities (dim 1): tensor([0.0647, 0.1151, 0.0647, 0.1151, 0.1151, 0.0647, 0.1151, 0.1151, 0.1151,\n",
      "        0.1151], grad_fn=<DivBackward0>)\n",
      "Probabilities (dim 2): tensor([0.0783, 0.0783, 0.1145, 0.1145, 0.0783, 0.0783, 0.1145, 0.1145, 0.1145,\n",
      "        0.1145], grad_fn=<DivBackward0>)\n",
      "Probabilities (dim 3): tensor([0.0784, 0.0980, 0.0000, 0.0980, 0.1569, 0.0980, 0.1569, 0.1569, 0.1569,\n",
      "        0.0000], grad_fn=<DivBackward0>)\n",
      "Probabilities (dim 4): tensor([0.1707, 0.0976, 0.0976, 0.1220, 0.0976, 0.0488, 0.0488, 0.1707, 0.0488,\n",
      "        0.0976], grad_fn=<DivBackward0>)\n",
      "Probabilities (dim 5): tensor([0.2222, 0.1111, 0.2222, 0.0000, 0.1111, 0.2778, 0.0000, 0.0556, 0.0000,\n",
      "        0.0000], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for dim, probs in enumerate(ziv_probs):\n",
    "    print(f\"Probabilities (dim {dim + 1}): {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities (Dim 1): 0.9999998807907104\n",
      "Probabilities (Dim 2): 1.0000001192092896\n",
      "Probabilities (Dim 3): 1.0\n",
      "Probabilities (Dim 4): 1.0000001192092896\n",
      "Probabilities (Dim 5): 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "for i, probs in enumerate(ziv_probs):\n",
    "    print(f\"Probabilities (Dim {i + 1}): {probs.sum().item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.4482],\n",
      "        [ 0.3387],\n",
      "        [-0.0010],\n",
      "        [ 0.1505],\n",
      "        [-0.3569],\n",
      "        [ 0.0705],\n",
      "        [-0.2231],\n",
      "        [ 0.2990],\n",
      "        [ 0.2601],\n",
      "        [ 0.6158]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.3932,  0.4345],\n",
      "        [ 0.4805,  0.4316],\n",
      "        [ 0.3137,  0.4136],\n",
      "        [ 0.3773,  0.0141],\n",
      "        [ 0.7026,  0.3844],\n",
      "        [ 0.0554,  0.2783],\n",
      "        [ 0.2746,  0.4723],\n",
      "        [ 0.6488, -0.1031],\n",
      "        [ 0.3484,  0.7957],\n",
      "        [-0.1551,  0.6804]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[1., 0.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 0.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.6477,  0.3905,  0.1454],\n",
      "        [-0.1050,  0.8465,  0.1021],\n",
      "        [ 0.6073,  0.6063, -0.0940],\n",
      "        [ 0.7762,  0.0377,  0.5855],\n",
      "        [-0.0972,  0.7313,  0.3791],\n",
      "        [ 0.1655,  0.3944,  0.7928],\n",
      "        [ 0.0737,  0.1732,  0.7704],\n",
      "        [ 0.1862,  0.2352,  0.4817],\n",
      "        [ 0.4872,  0.7355,  0.3801],\n",
      "        [-0.0309,  0.7352,  0.0894]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.],\n",
      "        [0., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.7528,  0.7407,  0.2901,  0.8493],\n",
      "        [ 0.3513,  0.5082,  0.5167,  0.3038],\n",
      "        [ 0.7474,  0.4460,  0.4605,  0.3124],\n",
      "        [ 0.8488,  0.0610,  0.8075,  0.8223],\n",
      "        [ 0.6349, -0.0060,  0.2039,  0.4989],\n",
      "        [ 0.8438,  0.8699,  0.8184,  0.0984],\n",
      "        [ 0.1531,  0.1441,  0.4814,  0.0654],\n",
      "        [ 0.7091,  0.5769,  0.4514,  0.1340],\n",
      "        [ 0.2302,  0.5023, -0.0176,  0.8233],\n",
      "        [ 0.8873,  0.7008,  0.7446,  0.6243]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[0., 0., 1., 1.],\n",
      "        [1., 0., 0., 1.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 0., 1.],\n",
      "        [1., 1., 1., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 0., 1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.3495,  0.6024,  0.3567,  0.6088,  0.7922],\n",
      "        [ 0.5664,  0.3934,  0.0984,  0.0614,  0.4181],\n",
      "        [ 0.0474,  0.3251,  0.1313,  0.7335,  0.3622],\n",
      "        [ 0.4177,  0.2267,  0.1830,  0.4652,  0.7925],\n",
      "        [ 0.2176,  0.7025,  0.6905, -0.0690,  0.3307],\n",
      "        [ 0.8569,  0.2109,  0.8263,  0.4886,  0.6147],\n",
      "        [ 0.3748,  0.3324,  0.9082, -0.0255, -0.0469],\n",
      "        [ 0.3728, -0.0412,  0.3229,  0.5874,  0.8155],\n",
      "        [ 0.1466,  0.5193,  0.6693,  0.3055,  0.5049],\n",
      "        [ 0.1520,  0.5947,  0.6907,  0.4723,  0.5430]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17563a100>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x17563a100>):\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 1., 0., 1., 0.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1.],\n",
      "        [0., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [1., 0., 1., 1., 1.],\n",
      "        [0., 1., 1., 0., 1.],\n",
      "        [1., 1., 1., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "Initial kernel for dimension 1:\n",
      "tensor([[ 0.4482],\n",
      "        [ 0.3387],\n",
      "        [-0.0010],\n",
      "        [ 0.1505],\n",
      "        [-0.3569],\n",
      "        [ 0.0705],\n",
      "        [-0.2231],\n",
      "        [ 0.2990],\n",
      "        [ 0.2601],\n",
      "        [ 0.6158]])\n",
      "\n",
      "Updated (binary) kernel for dimension 1 after the forward pass:\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.]])\n",
      "\n",
      "Difference between initial and updated (binary) kernel (dim 1):\n",
      "tensor([[ 0.5518],\n",
      "        [-0.3387],\n",
      "        [ 0.0010],\n",
      "        [-0.1505],\n",
      "        [ 0.3569],\n",
      "        [-0.0705],\n",
      "        [ 0.2231],\n",
      "        [-0.2990],\n",
      "        [-0.2601],\n",
      "        [ 0.3842]])\n"
     ]
    }
   ],
   "source": [
    "# Save a copy of the initial kernel for dimension 1\n",
    "initial_dim1_kernel = seq_models[0].p.clone().detach()\n",
    "\n",
    "# Perform the forward pass to generate the binary kernel\n",
    "binary_kernels = [seq_model() for seq_model in seq_models]\n",
    "\n",
    "# Get the updated binary kernel for dimension 1\n",
    "updated_dim1_binary_kernel = binary_kernels[0].detach()\n",
    "\n",
    "# Compare the initial kernel with the updated (binary) kernel after the forward pass\n",
    "print(\"\\nInitial kernel for dimension 1:\")\n",
    "print(initial_dim1_kernel)\n",
    "\n",
    "print(\"\\nUpdated (binary) kernel for dimension 1 after the forward pass:\")\n",
    "print(updated_dim1_binary_kernel)\n",
    "\n",
    "# Difference between the initial kernel and the updated (binary) kernel\n",
    "difference = updated_dim1_binary_kernel - initial_dim1_kernel\n",
    "print(\"\\nDifference between initial and updated (binary) kernel (dim 1):\")\n",
    "print(difference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.4482],\n",
      "        [ 0.3387],\n",
      "        [-0.0010],\n",
      "        [ 0.1505],\n",
      "        [-0.3569],\n",
      "        [ 0.0705],\n",
      "        [-0.2231],\n",
      "        [ 0.2990],\n",
      "        [ 0.2601],\n",
      "        [ 0.6158]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x17550fcd0>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x175500e50>):\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.3932,  0.4345],\n",
      "        [ 0.4805,  0.4316],\n",
      "        [ 0.3137,  0.4136],\n",
      "        [ 0.3773,  0.0141],\n",
      "        [ 0.7026,  0.3844],\n",
      "        [ 0.0554,  0.2783],\n",
      "        [ 0.2746,  0.4723],\n",
      "        [ 0.6488, -0.1031],\n",
      "        [ 0.3484,  0.7957],\n",
      "        [-0.1551,  0.6804]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x175500e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x175500e50>):\n",
      "tensor([[0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.6477,  0.3905,  0.1454],\n",
      "        [-0.1050,  0.8465,  0.1021],\n",
      "        [ 0.6073,  0.6063, -0.0940],\n",
      "        [ 0.7762,  0.0377,  0.5855],\n",
      "        [-0.0972,  0.7313,  0.3791],\n",
      "        [ 0.1655,  0.3944,  0.7928],\n",
      "        [ 0.0737,  0.1732,  0.7704],\n",
      "        [ 0.1862,  0.2352,  0.4817],\n",
      "        [ 0.4872,  0.7355,  0.3801],\n",
      "        [-0.0309,  0.7352,  0.0894]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x175500e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x175500e50>):\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [0., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.7528,  0.7407,  0.2901,  0.8493],\n",
      "        [ 0.3513,  0.5082,  0.5167,  0.3038],\n",
      "        [ 0.7474,  0.4460,  0.4605,  0.3124],\n",
      "        [ 0.8488,  0.0610,  0.8075,  0.8223],\n",
      "        [ 0.6349, -0.0060,  0.2039,  0.4989],\n",
      "        [ 0.8438,  0.8699,  0.8184,  0.0984],\n",
      "        [ 0.1531,  0.1441,  0.4814,  0.0654],\n",
      "        [ 0.7091,  0.5769,  0.4514,  0.1340],\n",
      "        [ 0.2302,  0.5023, -0.0176,  0.8233],\n",
      "        [ 0.8873,  0.7008,  0.7446,  0.6243]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x175500e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x175500e50>):\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 1., 0., 1.],\n",
      "        [1., 0., 1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Parameter p (before forward, requires_grad=True):\n",
      "Parameter containing:\n",
      "tensor([[ 0.3495,  0.6024,  0.3567,  0.6088,  0.7922],\n",
      "        [ 0.5664,  0.3934,  0.0984,  0.0614,  0.4181],\n",
      "        [ 0.0474,  0.3251,  0.1313,  0.7335,  0.3622],\n",
      "        [ 0.4177,  0.2267,  0.1830,  0.4652,  0.7925],\n",
      "        [ 0.2176,  0.7025,  0.6905, -0.0690,  0.3307],\n",
      "        [ 0.8569,  0.2109,  0.8263,  0.4886,  0.6147],\n",
      "        [ 0.3748,  0.3324,  0.9082, -0.0255, -0.0469],\n",
      "        [ 0.3728, -0.0412,  0.3229,  0.5874,  0.8155],\n",
      "        [ 0.1466,  0.5193,  0.6693,  0.3055,  0.5049],\n",
      "        [ 0.1520,  0.5947,  0.6907,  0.4723,  0.5430]], requires_grad=True)\n",
      "Logits grad_fn: <StackBackward0 object at 0x175500e50>\n",
      "Output S (grad_fn=<SelectBackward0 object at 0x175500e50>):\n",
      "tensor([[0., 1., 1., 1., 0.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1.]], grad_fn=<SelectBackward0>)\n",
      "\n",
      "--- Comparison between initial and updated kernels ---\n",
      "Dimension 1:\n",
      "Initial Kernel:\n",
      "tensor([[ 0.4482],\n",
      "        [ 0.3387],\n",
      "        [-0.0010],\n",
      "        [ 0.1505],\n",
      "        [-0.3569],\n",
      "        [ 0.0705],\n",
      "        [-0.2231],\n",
      "        [ 0.2990],\n",
      "        [ 0.2601],\n",
      "        [ 0.6158]])\n",
      "Updated (binary) Kernel:\n",
      "tensor([[1.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<SelectBackward0>)\n",
      "Difference:\n",
      "tensor([[ 0.5518],\n",
      "        [-0.3387],\n",
      "        [ 0.0010],\n",
      "        [-0.1505],\n",
      "        [ 0.3569],\n",
      "        [-0.0705],\n",
      "        [ 0.2231],\n",
      "        [-0.2990],\n",
      "        [-0.2601],\n",
      "        [-0.6158]], grad_fn=<SubBackward0>)\n",
      "Dimension 2:\n",
      "Initial Kernel:\n",
      "tensor([[ 0.3932,  0.4345],\n",
      "        [ 0.4805,  0.4316],\n",
      "        [ 0.3137,  0.4136],\n",
      "        [ 0.3773,  0.0141],\n",
      "        [ 0.7026,  0.3844],\n",
      "        [ 0.0554,  0.2783],\n",
      "        [ 0.2746,  0.4723],\n",
      "        [ 0.6488, -0.1031],\n",
      "        [ 0.3484,  0.7957],\n",
      "        [-0.1551,  0.6804]])\n",
      "Updated (binary) Kernel:\n",
      "tensor([[0., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 1.],\n",
      "        [0., 1.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 1.],\n",
      "        [0., 1.]], grad_fn=<SelectBackward0>)\n",
      "Difference:\n",
      "tensor([[-0.3932,  0.5655],\n",
      "        [ 0.5195,  0.5684],\n",
      "        [ 0.6863, -0.4136],\n",
      "        [-0.3773,  0.9859],\n",
      "        [ 0.2974,  0.6156],\n",
      "        [-0.0554,  0.7217],\n",
      "        [-0.2746, -0.4723],\n",
      "        [-0.6488,  0.1031],\n",
      "        [-0.3484,  0.2043],\n",
      "        [ 0.1551,  0.3196]], grad_fn=<SubBackward0>)\n",
      "Dimension 3:\n",
      "Initial Kernel:\n",
      "tensor([[ 0.6477,  0.3905,  0.1454],\n",
      "        [-0.1050,  0.8465,  0.1021],\n",
      "        [ 0.6073,  0.6063, -0.0940],\n",
      "        [ 0.7762,  0.0377,  0.5855],\n",
      "        [-0.0972,  0.7313,  0.3791],\n",
      "        [ 0.1655,  0.3944,  0.7928],\n",
      "        [ 0.0737,  0.1732,  0.7704],\n",
      "        [ 0.1862,  0.2352,  0.4817],\n",
      "        [ 0.4872,  0.7355,  0.3801],\n",
      "        [-0.0309,  0.7352,  0.0894]])\n",
      "Updated (binary) Kernel:\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [0., 1., 0.]], grad_fn=<SelectBackward0>)\n",
      "Difference:\n",
      "tensor([[-0.6477,  0.6095, -0.1454],\n",
      "        [ 0.1050,  0.1535, -0.1021],\n",
      "        [-0.6073,  0.3937,  0.0940],\n",
      "        [-0.7762, -0.0377,  0.4145],\n",
      "        [ 0.0972,  0.2687, -0.3791],\n",
      "        [-0.1655,  0.6056,  0.2072],\n",
      "        [-0.0737, -0.1732,  0.2296],\n",
      "        [-0.1862, -0.2352, -0.4817],\n",
      "        [ 0.5128,  0.2645, -0.3801],\n",
      "        [ 0.0309,  0.2648, -0.0894]], grad_fn=<SubBackward0>)\n",
      "Dimension 4:\n",
      "Initial Kernel:\n",
      "tensor([[ 0.7528,  0.7407,  0.2901,  0.8493],\n",
      "        [ 0.3513,  0.5082,  0.5167,  0.3038],\n",
      "        [ 0.7474,  0.4460,  0.4605,  0.3124],\n",
      "        [ 0.8488,  0.0610,  0.8075,  0.8223],\n",
      "        [ 0.6349, -0.0060,  0.2039,  0.4989],\n",
      "        [ 0.8438,  0.8699,  0.8184,  0.0984],\n",
      "        [ 0.1531,  0.1441,  0.4814,  0.0654],\n",
      "        [ 0.7091,  0.5769,  0.4514,  0.1340],\n",
      "        [ 0.2302,  0.5023, -0.0176,  0.8233],\n",
      "        [ 0.8873,  0.7008,  0.7446,  0.6243]])\n",
      "Updated (binary) Kernel:\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 1., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 1.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 0., 1., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [0., 1., 0., 1.],\n",
      "        [1., 0., 1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Difference:\n",
      "tensor([[-0.7528, -0.7407, -0.2901, -0.8493],\n",
      "        [-0.3513, -0.5082,  0.4833,  0.6962],\n",
      "        [ 0.2526, -0.4460,  0.5395, -0.3124],\n",
      "        [ 0.1512, -0.0610,  0.1925,  0.1777],\n",
      "        [ 0.3651,  0.0060,  0.7961,  0.5011],\n",
      "        [ 0.1562, -0.8699,  0.1816, -0.0984],\n",
      "        [ 0.8469, -0.1441,  0.5186, -0.0654],\n",
      "        [ 0.2909,  0.4231, -0.4514, -0.1340],\n",
      "        [-0.2302,  0.4977,  0.0176,  0.1767],\n",
      "        [ 0.1127, -0.7008,  0.2554,  0.3757]], grad_fn=<SubBackward0>)\n",
      "Dimension 5:\n",
      "Initial Kernel:\n",
      "tensor([[ 0.3495,  0.6024,  0.3567,  0.6088,  0.7922],\n",
      "        [ 0.5664,  0.3934,  0.0984,  0.0614,  0.4181],\n",
      "        [ 0.0474,  0.3251,  0.1313,  0.7335,  0.3622],\n",
      "        [ 0.4177,  0.2267,  0.1830,  0.4652,  0.7925],\n",
      "        [ 0.2176,  0.7025,  0.6905, -0.0690,  0.3307],\n",
      "        [ 0.8569,  0.2109,  0.8263,  0.4886,  0.6147],\n",
      "        [ 0.3748,  0.3324,  0.9082, -0.0255, -0.0469],\n",
      "        [ 0.3728, -0.0412,  0.3229,  0.5874,  0.8155],\n",
      "        [ 0.1466,  0.5193,  0.6693,  0.3055,  0.5049],\n",
      "        [ 0.1520,  0.5947,  0.6907,  0.4723,  0.5430]])\n",
      "Updated (binary) Kernel:\n",
      "tensor([[0., 1., 1., 1., 0.],\n",
      "        [0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 1., 0.],\n",
      "        [1., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0., 1.],\n",
      "        [1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [0., 1., 1., 1., 1.]], grad_fn=<SelectBackward0>)\n",
      "Difference:\n",
      "tensor([[-0.3495,  0.3976,  0.6433,  0.3912, -0.7922],\n",
      "        [-0.5664,  0.6066, -0.0984, -0.0614,  0.5819],\n",
      "        [ 0.9526, -0.3251,  0.8687,  0.2665, -0.3622],\n",
      "        [ 0.5823, -0.2267, -0.1830,  0.5348, -0.7925],\n",
      "        [-0.2176, -0.7025, -0.6905,  1.0690, -0.3307],\n",
      "        [-0.8569, -0.2109,  0.1737, -0.4886,  0.3853],\n",
      "        [ 0.6252, -0.3324, -0.9082,  0.0255,  0.0469],\n",
      "        [-0.3728,  0.0412, -0.3229,  0.4126,  0.1845],\n",
      "        [ 0.8534,  0.4807,  0.3307,  0.6945,  0.4951],\n",
      "        [-0.1520,  0.4053,  0.3093,  0.5277,  0.4570]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Save a copy of the initial kernels\n",
    "initial_kernels = [seq_model.p.clone().detach() for seq_model in seq_models]\n",
    "\n",
    "# Perform the forward pass to generate the updated binary kernels\n",
    "binary_kernels = [seq_model() for seq_model in seq_models]\n",
    "\n",
    "# Comparison between initial and updated kernels\n",
    "print(\"\\n--- Comparison between initial and updated kernels ---\")\n",
    "for dim_index, (initial_kernel, binary_kernel) in enumerate(zip(initial_kernels, binary_kernels)):\n",
    "    print(f\"Dimension {dim_index + 1}:\")\n",
    "    print(f\"Initial Kernel:\\n{initial_kernel}\")\n",
    "    print(f\"Updated (binary) Kernel:\\n{binary_kernel}\")\n",
    "    difference = binary_kernel - initial_kernel\n",
    "    print(f\"Difference:\\n{difference}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
